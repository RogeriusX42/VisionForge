# 🧠 VisionForge – AI-102 Vision & Video Lab

**VisionForge** é um laboratório multimodal baseado em visão computacional e inteligência artificial, com foco em **acessibilidade**, **cognição aumentada**, **educação inclusiva** e integração com plataformas como **Xbox**, **Samsung**, **Tizen** e **PlayStation**.

Este projeto faz parte de uma série de módulos iterativos voltados à prática real do conteúdo da certificação **AI-102 (Microsoft Azure AI Engineer)**, utilizando interfaces naturais para expandir as possibilidades humanas.

---

## 🎯 Objetivos do Projeto

- Criar módulos acessíveis e multiplataforma usando visão computacional
- Aplicar técnicas de reconhecimento em tempo real para suporte inclusivo
- Submeter e adaptar o projeto para Xbox, Samsung, Tizen e PlayStation
- Estimular o uso educativo de tecnologias de IA com código aberto

---

## 🔁 Estrutura Iterativa – Milestones

### 🖐️ 1. HandCount Decoder
> **Descrição:** Reconhecimento de dedos (0–5) com exibição em Decimal e Binário.  
> **Tecnologias:** JavaScript + MediaPipe Hands  
> **Resultado:** Interface leve para jogos, contagem e comandos por gesto.

---

### 😊 2. EmotiScan
> **Descrição:** Detecção de expressões faciais e classificação emocional (alegria, raiva, medo, surpresa, etc.)  
> **Tecnologias:** Azure Face API ou CNN via OpenCV  
> **Resultado:** Feedback emocional visual e textual em tempo real.

---

### 🤟 3. LibrasTranscribe
> **Descrição:** Tradução de sinais da **Língua Brasileira de Sinais (LIBRAS)** para texto/voz em português.  
> **Tecnologias:** MediaPipe Holistic + modelo customizado para LIBRAS  
> **Resultado:** Inclusão comunicacional para comunidades surdas.

---

### 🌐 4. ISL Bridge
> **Descrição:** Tradução de sinais nacionais (LIBRAS) para **Linguagem Internacional de Sinais (ISL)**.  
> **Tecnologias:** Reconhecimento semântico de padrões + mapeamento universal  
> **Resultado:** Comunicação transfronteiriça por meio de gestos universais.

---

### 🧠👁️ 5. EchoVision.AI
> **Descrição:** Assistente multimodal que combina voz, vídeo e IA para descrição de cenas, leitura labial e resposta conversacional.  
> **Tecnologias:** Azure Cognitive Services (Vision + Speech + TTS)  
> **Resultado:** Apoio para deficientes visuais/auditivos em tempo real.

---

## 🔧 Tecnologias Utilizadas

- HTML5, JavaScript, CSS3
- MediaPipe, TensorFlow Lite, ONNX
- Azure Cognitive Services (AI-102)
- GitHub Codespaces / Replit / Codesandbox
- Compatibilidade com: Web, Android, Smart TV, Xbox Edge, DevKit PlayStation

---

## 📡 Plataformas-Alvo para Submissão

- [x] ID@Xbox – Xbox Developer Program  
- [x] Samsung Developer Program  
- [x] Tizen Developer Program (Smart TV & Wearables)  
- [x] PlayStation Partners / Indie Dev

---

## 🧪 Status Atual

| Milestone | Progresso |
|-----------|-----------|
| HandCount Decoder | ✅ 86% – protótipo funcional (Replit) |
| EmotiScan | 🔜 Em desenvolvimento |
| LibrasTranscribe | 🔜 Planejamento de dataset |
| ISL Bridge | ⏳ Projeção futura |
| EchoVision.AI | ⏳ Design em construção |

---

## 📜 Licença

Este projeto está sob a licença MIT.  
Livre para fins educacionais, acadêmicos e de acessibilidade com atribuição.

---

> “Quando a câmera vê com compaixão, o código se torna ponte para a alma.”
